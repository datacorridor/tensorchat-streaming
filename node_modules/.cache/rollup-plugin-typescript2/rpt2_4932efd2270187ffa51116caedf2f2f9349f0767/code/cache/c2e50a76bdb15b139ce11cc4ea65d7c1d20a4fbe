{"code":"import { __awaiter, __generator } from \"tslib\";\nvar TensorChatStreaming = /** @class */ (function () {\n    function TensorChatStreaming(config) {\n        this.throttleTimers = new Map();\n        this.apiKey = config.apiKey;\n        this.baseUrl = config.baseUrl || 'https://api.tensorchat.ai';\n        this.throttleMs = config.throttleMs || 50; // Default 50ms throttle\n    }\n    /**\n     * Throttle function calls to prevent UI flooding\n     */\n    TensorChatStreaming.prototype.throttle = function (key, fn) {\n        var _this = this;\n        var args = [];\n        for (var _i = 2; _i < arguments.length; _i++) {\n            args[_i - 2] = arguments[_i];\n        }\n        if (this.throttleTimers.has(key)) {\n            clearTimeout(this.throttleTimers.get(key));\n        }\n        var timeoutId = window.setTimeout(function () {\n            fn.apply(void 0, args);\n            _this.throttleTimers.delete(key);\n        }, this.throttleMs);\n        this.throttleTimers.set(key, timeoutId);\n    };\n    /**\n     * Stream process tensors with real-time callbacks\n     */\n    TensorChatStreaming.prototype.streamProcess = function (request_1) {\n        return __awaiter(this, arguments, void 0, function (request, callbacks) {\n            var onStart, onProgress, onSearchProgress, onSearchComplete, onTensorChunk, onTensorComplete, onTensorError, onComplete, onError, response, reader, decoder, buffer, _a, done, value, lineEnd, line, data, error_1;\n            var _b, _c;\n            if (callbacks === void 0) { callbacks = {}; }\n            return __generator(this, function (_d) {\n                switch (_d.label) {\n                    case 0:\n                        onStart = callbacks.onStart, onProgress = callbacks.onProgress, onSearchProgress = callbacks.onSearchProgress, onSearchComplete = callbacks.onSearchComplete, onTensorChunk = callbacks.onTensorChunk, onTensorComplete = callbacks.onTensorComplete, onTensorError = callbacks.onTensorError, onComplete = callbacks.onComplete, onError = callbacks.onError;\n                        _d.label = 1;\n                    case 1:\n                        _d.trys.push([1, 9, , 10]);\n                        return [4 /*yield*/, fetch(\"\".concat(this.baseUrl, \"/streamProcess\"), {\n                                method: 'POST',\n                                headers: {\n                                    'Content-Type': 'application/json',\n                                    'Authorization': \"Bearer \".concat(this.apiKey),\n                                    'x-api-key': this.apiKey,\n                                },\n                                body: JSON.stringify(request),\n                            })];\n                    case 2:\n                        response = _d.sent();\n                        if (!response.ok) {\n                            throw new Error(\"HTTP \".concat(response.status, \": \").concat(response.statusText));\n                        }\n                        reader = (_b = response.body) === null || _b === void 0 ? void 0 : _b.getReader();\n                        if (!reader) {\n                            throw new Error('Response body is not readable');\n                        }\n                        decoder = new TextDecoder();\n                        buffer = '';\n                        _d.label = 3;\n                    case 3:\n                        _d.trys.push([3, , 7, 8]);\n                        _d.label = 4;\n                    case 4:\n                        if (!true) return [3 /*break*/, 6];\n                        return [4 /*yield*/, reader.read()];\n                    case 5:\n                        _a = _d.sent(), done = _a.done, value = _a.value;\n                        if (done)\n                            return [3 /*break*/, 6];\n                        buffer += decoder.decode(value, { stream: true });\n                        // Process complete lines\n                        while (buffer.includes('\\n\\n')) {\n                            lineEnd = buffer.indexOf('\\n\\n');\n                            line = buffer.slice(0, lineEnd);\n                            buffer = buffer.slice(lineEnd + 2);\n                            if (line.startsWith('data: ')) {\n                                try {\n                                    data = JSON.parse(line.slice(6));\n                                    switch (data.type) {\n                                        case 'start':\n                                            console.log(\"\\uD83D\\uDE80 Starting \".concat(data.totalTensors, \" tensors with \").concat(data.model));\n                                            onStart === null || onStart === void 0 ? void 0 : onStart(data);\n                                            break;\n                                        case 'progress':\n                                            console.log(\"\\u23F3 Processing tensor \".concat(data.index, \"...\"));\n                                            onProgress === null || onProgress === void 0 ? void 0 : onProgress(data);\n                                            break;\n                                        case 'search_progress':\n                                            console.log(\"\\uD83D\\uDD0D Searching for tensor \".concat(data.index, \"...\"));\n                                            onSearchProgress === null || onSearchProgress === void 0 ? void 0 : onSearchProgress(data);\n                                            break;\n                                        case 'search_complete':\n                                            console.log(\"\\u2705 Search completed for tensor \".concat(data.index));\n                                            onSearchComplete === null || onSearchComplete === void 0 ? void 0 : onSearchComplete(data);\n                                            break;\n                                        case 'tensor_chunk':\n                                            console.log(\"\\uD83D\\uDCDD Tensor \".concat(data.index, \" chunk received\"));\n                                            // Throttle chunk updates to prevent UI flooding\n                                            if (onTensorChunk && data.index !== undefined) {\n                                                this.throttle(\"chunk-\".concat(data.index), onTensorChunk, data);\n                                            }\n                                            break;\n                                        case 'tensor_complete':\n                                            console.log(\"\\u2705 Tensor \".concat(data.index, \" completed\"));\n                                            onTensorComplete === null || onTensorComplete === void 0 ? void 0 : onTensorComplete(data);\n                                            break;\n                                        case 'tensor_error':\n                                            console.warn(\"\\u274C Tensor \".concat(data.index, \" failed: \").concat((_c = data.result) === null || _c === void 0 ? void 0 : _c.error));\n                                            onTensorError === null || onTensorError === void 0 ? void 0 : onTensorError(data);\n                                            break;\n                                        case 'complete':\n                                            console.log(\"\\uD83C\\uDF89 All tensors completed!\");\n                                            onComplete === null || onComplete === void 0 ? void 0 : onComplete(data);\n                                            return [2 /*return*/];\n                                        case 'error':\n                                        case 'fatal_error':\n                                            throw new Error(data.error || data.details || 'Streaming error');\n                                    }\n                                }\n                                catch (parseError) {\n                                    console.warn('Failed to parse streaming data:', line, parseError);\n                                }\n                            }\n                        }\n                        return [3 /*break*/, 4];\n                    case 6: return [3 /*break*/, 8];\n                    case 7:\n                        reader.releaseLock();\n                        return [7 /*endfinally*/];\n                    case 8: return [3 /*break*/, 10];\n                    case 9:\n                        error_1 = _d.sent();\n                        console.error('Streaming error:', error_1);\n                        onError === null || onError === void 0 ? void 0 : onError(error_1 instanceof Error ? error_1 : new Error(String(error_1)));\n                        return [3 /*break*/, 10];\n                    case 10: return [2 /*return*/];\n                }\n            });\n        });\n    };\n    /**\n     * Process a single tensor (non-streaming)\n     */\n    TensorChatStreaming.prototype.processSingle = function (request) {\n        return __awaiter(this, void 0, void 0, function () {\n            var response;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0: return [4 /*yield*/, fetch(\"\".concat(this.baseUrl, \"/process\"), {\n                            method: 'POST',\n                            headers: {\n                                'Content-Type': 'application/json',\n                                'Authorization': \"Bearer \".concat(this.apiKey),\n                                'x-api-key': this.apiKey,\n                            },\n                            body: JSON.stringify(request),\n                        })];\n                    case 1:\n                        response = _a.sent();\n                        if (!response.ok) {\n                            throw new Error(\"HTTP \".concat(response.status, \": \").concat(response.statusText));\n                        }\n                        return [2 /*return*/, response.json()];\n                }\n            });\n        });\n    };\n    /**\n     * Clean up any pending throttled calls\n     */\n    TensorChatStreaming.prototype.destroy = function () {\n        this.throttleTimers.forEach(function (timerId) { return clearTimeout(timerId); });\n        this.throttleTimers.clear();\n    };\n    return TensorChatStreaming;\n}());\nexport { TensorChatStreaming };\n","references":["D:/Git/tensorchat-streaming/src/types.ts"],"dts":{"name":"D:/Git/tensorchat-streaming/dist/TensorChatStreaming.d.ts","writeByteOrderMark":false,"text":"import { StreamRequest, StreamCallbacks, TensorChatConfig } from './types';\nexport declare class TensorChatStreaming {\n    private apiKey;\n    private baseUrl;\n    private throttleMs;\n    private throttleTimers;\n    constructor(config: TensorChatConfig);\n    /**\n     * Throttle function calls to prevent UI flooding\n     */\n    private throttle;\n    /**\n     * Stream process tensors with real-time callbacks\n     */\n    streamProcess(request: StreamRequest, callbacks?: StreamCallbacks): Promise<void>;\n    /**\n     * Process a single tensor (non-streaming)\n     */\n    processSingle(request: StreamRequest): Promise<any>;\n    /**\n     * Clean up any pending throttled calls\n     */\n    destroy(): void;\n}\n"}}
